{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WMGpndMtQjKX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test)= fashion_mnist.load_data()      #loading the dataset\n",
        "\n",
        "x_train= x_train.reshape(x_train.shape[0], -1)                        #reshape the x into (60000, 784)\n",
        "#y= y_train                                                            #y is already a 1D array\n",
        "\n",
        "x_train= x_train.reshape(x_train.shape[0], -1)/255.0                        #normalizing x\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0                   # Normalize\n",
        "\n",
        "def display_dataset_info(x_train, y_train, x_test, y_test):\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Fashion MNIST Dataset Details\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Training Samples: {x_train.shape[0]}\")\n",
        "    print(f\"Test Samples: {x_test.shape[0]}\")\n",
        "    print(f\"Input Features: {x_train.shape[1]}\")\n",
        "    print(f\"Number of Classes: {len(np.unique(y_train))}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "# Call the function\n",
        "display_dataset_info(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "zzFSl9gOXtCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0578bf0-b764-4878-e4b4-9624b3fe666d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "========================================\n",
            "Fashion MNIST Dataset Details\n",
            "========================================\n",
            "Training Samples: 60000\n",
            "Test Samples: 10000\n",
            "Input Features: 784\n",
            "Number of Classes: 10\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Normalize\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n"
      ],
      "metadata": {
        "id": "SZkwSAEFQqsA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer Class\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-4, beta=0.9, beta1= 0.9, beta2= 0.999):\n",
        "      self.learning_rate= learning_rate\n",
        "      self.epsilon= epsilon\n",
        "      self.beta= beta\n",
        "      self.beta1= beta1\n",
        "      self.beta2= beta2\n",
        "\n",
        "\n",
        "    def sgd(self, parameters, grads, prev_u= None):\n",
        "      for t in parameters.keys():\n",
        "        parameters[t] -= self.learning_rate* grads[ t]\n",
        "\n",
        "      return parameters, prev_u\n",
        "\n",
        "\n",
        "\n",
        "    def momentum_gradient_descent(self, parameters, grads, prev_u=None, beta=0.9):\n",
        "      if prev_u is None:\n",
        "        prev_u = {key: np.zeros_like(value) for key, value in parameters.items()}  # Initialize with zeros (u(-1)= 0)\n",
        "\n",
        "      for t in parameters.keys():\n",
        "          prev_u[t]= beta * prev_u[t] +  grads[t]\n",
        "          parameters[t] -= self.learning_rate* prev_u[t]                     #prev_u[t] accumuklates the history vector\n",
        "\n",
        "      return parameters, prev_u\n",
        "\n",
        "\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, parameters, grads, prev_u= None, beta=0.9):\n",
        "      if prev_u is None:\n",
        "        prev_u= {t: np.zeros_like(value) for t, value in parameters.items()}     # initialize u(-1)= 0\n",
        "\n",
        "      lookahead_params = {t: parameters[t] - beta * prev_u[t] for t in parameters.keys()}\n",
        "\n",
        "      temp_grads = grads\n",
        "      for t in parameters.keys():\n",
        "        prev_u[t]= beta * prev_u[t] + self.learning_rate * temp_grads[t]         #prev_u[t] accumuklates the history vector\n",
        "        parameters[t] -= prev_u[t]\n",
        "      return parameters, prev_u\n",
        "\n",
        "\n",
        "    def rmsprop(self, parameters, grads, prev_u= None, beta= 0.9):\n",
        "      if prev_u is None:\n",
        "        prev_u = {t: np.zeros_like(value) for t, value in parameters.items()}    # initialize u(-1)= 0\n",
        "\n",
        "      for t in parameters.keys():\n",
        "        prev_u[t]= beta * prev_u[t] + (1 - beta) * grads[t] ** 2\n",
        "\n",
        "        parameters[t] -= (self.learning_rate / (np.sqrt(prev_u[t]) + self.epsilon)) * grads[t]\n",
        "\n",
        "      return parameters, prev_u\n",
        "       #adam\n",
        "\n",
        "    def adam(self, parameters, grads, prev_u=None):\n",
        "      if prev_u is None or \"step\" not in prev_u:           #initialization\n",
        "        prev_u = {\"m\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"v\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"step\": 0\n",
        "                  }\n",
        "\n",
        "      prev_u[\"step\"] += 1                                   #time step increment\n",
        "      step= prev_u[\"step\"]\n",
        "\n",
        "      for t in parameters.keys():\n",
        "        prev_u[\"m\"][t]=  self.beta1 * prev_u[\"m\"][t] + (1 - self.beta1) * grads[t]                  #update rule\n",
        "        prev_u[\"v\"][t]= self.beta2 * prev_u[\"v\"][t] + (1 - self.beta2) * (grads[t] ** 2)\n",
        "\n",
        "        m_hat= prev_u[\"m\"][t] / (1 - self.beta1 ** step)\n",
        "        v_hat = prev_u[\"v\"][t] / (1 - self.beta2 ** step)\n",
        "\n",
        "        parameters[t] -= (self.learning_rate * m_hat) / (np.sqrt(v_hat) + self.epsilon)          #parameter update\n",
        "\n",
        "\n",
        "      return parameters, prev_u\n",
        "\n",
        "\n",
        "    #nadam\n",
        "    def nadam(self, parameters, grads, prev_u=None):\n",
        "      if prev_u is None or \"step\" not in prev_u:                   #initialization\n",
        "        prev_u=  { \"m\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"v\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"step\": 0\n",
        "             }\n",
        "      prev_u[\"step\"] += 1                                           #time step increment\n",
        "      step= prev_u[\"step\"]\n",
        "\n",
        "      for t in parameters.keys():\n",
        "        prev_u[\"m\"][t]=  self.beta1 * prev_u[\"m\"][t] + (1 - self.beta1) * grads[t]               #update rule\n",
        "        prev_u[\"v\"][t] = self.beta2 * prev_u[\"v\"][t] + (1 - self.beta2) * (grads[t] ** 2)\n",
        "\n",
        "        m_hat= prev_u[\"m\"][t] / (1 - self.beta1 ** step)\n",
        "        v_hat= prev_u[\"v\"][t] / (1 - self.beta2 ** step)\n",
        "\n",
        "        nadam_update= (self.beta1 * m_hat + (1 - self.beta1) * grads[t] / (1 - self.beta1 ** step))     #\n",
        "        parameters[t] -= (self.learning_rate / (np.sqrt(v_hat) + self.epsilon)) * nadam_update          #parameter update\n",
        "\n",
        "\n",
        "      return parameters, prev_u\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XWpVrHjNQwy3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.parameters = self.initialize_weights_bias(layer_sizes)\n",
        "        self.layer_sizes = layer_sizes\n",
        "\n",
        "    def initialize_weights_bias(self, layer_size):\n",
        "        parameters = {}\n",
        "        L = len(layer_size) - 1  # Exclude the input layer\n",
        "\n",
        "        for i in range(1, L + 1):\n",
        "            parameters[f\"W{i}\"] = np.random.randn(layer_size[i], layer_size[i - 1])  # Initialize W randomly\n",
        "            parameters[f\"b{i}\"] = np.random.randn(layer_size[i], 1)                 # Initialize b randomly, column vector\n",
        "\n",
        "        return parameters\n",
        "\n",
        "    def forward_propagation(self, x_train, y_train, num_of_hidden_layers, num_of_neurons):\n",
        "        N = len(np.unique(y_train))\n",
        "        input_dimension = x_train.shape[1]                  # Number of features per sample\n",
        "        layer_size = [input_dimension] + [num_of_neurons] * num_of_hidden_layers + [N]\n",
        "\n",
        "        parameters = self.initialize_weights_bias(layer_size)  # Initialize weights & biases\n",
        "\n",
        "        pre_activations = {}\n",
        "        activations = {\"h0\": x_train.T}                        # Input layer activation\n",
        "\n",
        "        for i in range(1, num_of_hidden_layers + 1):\n",
        "            a_i = np.dot(self.parameters[f\"W{i}\"], activations[f\"h{i-1}\"]) + self.parameters[f\"b{i}\"]\n",
        "            h_i = sigmoid(a_i)\n",
        "\n",
        "            pre_activations[f\"a{i}\"] = a_i\n",
        "            activations[f\"h{i}\"] = h_i\n",
        "\n",
        "        # Output layer forward pass\n",
        "        L = num_of_hidden_layers + 1\n",
        "        a_L = np.dot(self.parameters[f\"W{L}\"], activations[f\"h{L - 1}\"]) + self.parameters[f\"b{L}\"]\n",
        "        h_L = softmax(a_L)                                   # Final output\n",
        "\n",
        "        pre_activations[f\"a{L}\"] = a_L\n",
        "        activations[f\"h{L}\"] = h_L\n",
        "\n",
        "        return h_L.T, pre_activations, activations\n",
        "\n",
        "    def train_test_split(self, x_train, y_train, test_size=0.2, random_state=42):\n",
        "        return train_test_split(x_train, y_train, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    def backpropagation(self, x_train, y_train, parameters, pre_activations, activations, optimizer, batch_size, prev_v):\n",
        "        N = len(np.unique(y_train))\n",
        "        y_one_hot = np.eye(N)[y_train].T\n",
        "        grads = {}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "\n",
        "        #talk to the output layer\n",
        "\n",
        "        nabla_a_L = activations[f\"h{L}\"] - y_one_hot                                    #gradient wrt the last layer\n",
        "        grads[f\"W{L}\"] = np.dot(nabla_a_L, activations[f\"h{L-1}\"].T) / batch_size\n",
        "        grads[f\"b{L}\"] = np.sum(nabla_a_L, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "        for k in range(L - 1, 0, -1):                                        #talk to the hidden layer\n",
        "            nabla_h_k = np.dot(parameters[f\"W{k+1}\"].T, nabla_a_L)                      #gradients wrt the hidden layers\n",
        "            nabla_a_k = nabla_h_k * sigmoid_derivative(pre_activations[f\"a{k}\"])\n",
        "            grads[f\"W{k}\"] = np.dot(nabla_a_k, activations[f\"h{k-1}\"].T) / batch_size   #gradient wrt the weights\n",
        "            grads[f\"b{k}\"] = np.sum(nabla_a_k, axis=1, keepdims=True) / batch_size      #gradient wrt the bias\n",
        "            nabla_a_L = nabla_a_k\n",
        "\n",
        "        self.parameters, prev_v = optimizer(self.parameters, grads, prev_v)\n",
        "        return parameters, prev_v\n",
        "     # training the model\n",
        "    def train_model(self, x_train, y_train, num_hidden_layers, num_neurons, optimizer_type, learning_rate, num_epochs, batch_size):\n",
        "      input_dim= x_train.shape[1]\n",
        "      num_classes = len(np.unique(y_train))\n",
        "      self.layer_sizes = [input_dim] + [num_neurons] * num_hidden_layers + [num_classes]\n",
        "      self.parameters = self.initialize_weights_bias(self.layer_sizes)\n",
        "      optimizer_instance = Optimizer(learning_rate)\n",
        "      prev_v = {key: np.zeros_like(value) for key, value in self.parameters.items()}\n",
        "      losses = []\n",
        "\n",
        "      if hasattr(optimizer_instance, optimizer_type):     #checking if the optimizer is present in the \"Optimizer \" class\n",
        "        optimizer = getattr(optimizer_instance, optimizer_type)\n",
        "      else:\n",
        "        raise ValueError(f\"Optimizer '{optimizer_type}' is not defined in Optimizer class.\")\n",
        "\n",
        "      for epoch in range(num_epochs):\n",
        "        y_hat, pre_activations, activations = self.forward_propagation(x_train, y_train, num_hidden_layers, num_neurons)\n",
        "        self.parameters, prev_v = self.backpropagation(x_train, y_train, self.parameters, pre_activations, activations, optimizer, batch_size, prev_v)\n",
        "\n",
        "\n",
        "        #  loss calculation\n",
        "\n",
        "        loss = -np.mean(np.log(y_hat[np.arange(len(y_train)), y_train]))\n",
        "\n",
        "\n",
        "\n",
        "        losses.append(loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "      # Compute final training accuracy\n",
        "      train_predictions = np.argmax(y_hat, axis=1)  # Get predicted class\n",
        "      train_accuracy = np.mean(train_predictions == y_train) * 100  # Compute accuracy\n",
        "      print(f\"Final Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "      plt.plot(losses)\n",
        "      plt.xlabel(\"Epochs\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      plt.title(\"Training Loss\")\n",
        "      plt.show()\n",
        "\n",
        "      return train_accuracy, losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def test_model(self, x_test, y_test, num_hidden_layers, num_neurons, batch_size):\n",
        "        input_dim = x_test.shape[1]\n",
        "        num_classes = len(np.unique(y_test))\n",
        "        self.layer_sizes = [input_dim] + [num_neurons] * num_hidden_layers + [num_classes]\n",
        "        y_hat, _, _ = self.forward_propagation(x_test, y_test, num_hidden_layers, num_neurons)\n",
        "\n",
        "        if y_hat.shape[0] != x_test.shape[0]:    #shape (n_samples, n_classes)\n",
        "          y_hat= y_hat.T\n",
        "\n",
        "        test_predictions = np.argmax(y_hat, axis=1)\n",
        "        min_length = min(len(test_predictions), len(y_test))\n",
        "\n",
        "\n",
        "        accuracy = np.mean(test_predictions[:min_length] == y_test[:min_length]) * 100\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "9gCWPBU_t4rT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "# Initialize  with hidden layers easily\n",
        "nn = NeuralNetwork([x_train.shape[1]] + [70] * 4 + [len(np.unique(y_train))])\n",
        "\n",
        "# Train model function call\n",
        "#train_acc, loss_history = nn.train_model(x_train, y_train, 4, 70, 'nesterov_accelerated_gradient_descent', 0.001, 250, 64)\n",
        "#train_acc, loss_history = nn.train_model(x_train, y_train, 4, 70, 'rmsprop', 0.001, 250, 64)\n",
        "#train_acc, loss_history = nn.train_model(x_train, y_train, 4, 70, 'nadam', 0.001, 250, 64)\n",
        "train_acc, loss_history = nn.train_model(x_train, y_train, 4, 70, 'adam', 0.001, 250, 64)\n",
        "\n",
        "# Compute test accuracy using the trained model\n",
        "test_acc = nn.test_model(x_test, y_test, 4, 70, batch_size=64)\n",
        "\n",
        "# Print final accuracies\n",
        "print(f\"Final Training Accuracy: {train_acc:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1X0tc-Dwf4aO",
        "outputId": "a990841f-305a-4ce0-c063-c18158a4cad2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250, Loss: 7.5795\n",
            "Epoch 2/250, Loss: 7.0798\n",
            "Epoch 3/250, Loss: 6.5965\n",
            "Epoch 4/250, Loss: 6.1333\n",
            "Epoch 5/250, Loss: 5.6955\n",
            "Epoch 6/250, Loss: 5.2902\n",
            "Epoch 7/250, Loss: 4.9250\n",
            "Epoch 8/250, Loss: 4.6063\n",
            "Epoch 9/250, Loss: 4.3369\n",
            "Epoch 10/250, Loss: 4.1152\n",
            "Epoch 11/250, Loss: 3.9349\n",
            "Epoch 12/250, Loss: 3.7867\n",
            "Epoch 13/250, Loss: 3.6611\n",
            "Epoch 14/250, Loss: 3.5499\n",
            "Epoch 15/250, Loss: 3.4474\n",
            "Epoch 16/250, Loss: 3.3498\n",
            "Epoch 17/250, Loss: 3.2557\n",
            "Epoch 18/250, Loss: 3.1645\n",
            "Epoch 19/250, Loss: 3.0766\n",
            "Epoch 20/250, Loss: 2.9929\n",
            "Epoch 21/250, Loss: 2.9142\n",
            "Epoch 22/250, Loss: 2.8412\n",
            "Epoch 23/250, Loss: 2.7741\n",
            "Epoch 24/250, Loss: 2.7128\n",
            "Epoch 25/250, Loss: 2.6565\n",
            "Epoch 26/250, Loss: 2.6042\n",
            "Epoch 27/250, Loss: 2.5546\n",
            "Epoch 28/250, Loss: 2.5068\n",
            "Epoch 29/250, Loss: 2.4597\n",
            "Epoch 30/250, Loss: 2.4128\n",
            "Epoch 31/250, Loss: 2.3657\n",
            "Epoch 32/250, Loss: 2.3184\n",
            "Epoch 33/250, Loss: 2.2710\n",
            "Epoch 34/250, Loss: 2.2241\n",
            "Epoch 35/250, Loss: 2.1778\n",
            "Epoch 36/250, Loss: 2.1328\n",
            "Epoch 37/250, Loss: 2.0894\n",
            "Epoch 38/250, Loss: 2.0479\n",
            "Epoch 39/250, Loss: 2.0085\n",
            "Epoch 40/250, Loss: 1.9714\n",
            "Epoch 41/250, Loss: 1.9365\n",
            "Epoch 42/250, Loss: 1.9038\n",
            "Epoch 43/250, Loss: 1.8731\n",
            "Epoch 44/250, Loss: 1.8443\n",
            "Epoch 45/250, Loss: 1.8172\n",
            "Epoch 46/250, Loss: 1.7917\n",
            "Epoch 47/250, Loss: 1.7675\n",
            "Epoch 48/250, Loss: 1.7446\n",
            "Epoch 49/250, Loss: 1.7227\n",
            "Epoch 50/250, Loss: 1.7018\n",
            "Epoch 51/250, Loss: 1.6817\n",
            "Epoch 52/250, Loss: 1.6623\n",
            "Epoch 53/250, Loss: 1.6435\n",
            "Epoch 54/250, Loss: 1.6252\n",
            "Epoch 55/250, Loss: 1.6073\n",
            "Epoch 56/250, Loss: 1.5898\n",
            "Epoch 57/250, Loss: 1.5726\n",
            "Epoch 58/250, Loss: 1.5557\n",
            "Epoch 59/250, Loss: 1.5391\n",
            "Epoch 60/250, Loss: 1.5228\n",
            "Epoch 61/250, Loss: 1.5069\n",
            "Epoch 62/250, Loss: 1.4912\n",
            "Epoch 63/250, Loss: 1.4759\n",
            "Epoch 64/250, Loss: 1.4609\n",
            "Epoch 65/250, Loss: 1.4463\n",
            "Epoch 66/250, Loss: 1.4321\n",
            "Epoch 67/250, Loss: 1.4182\n",
            "Epoch 68/250, Loss: 1.4046\n",
            "Epoch 69/250, Loss: 1.3914\n",
            "Epoch 70/250, Loss: 1.3785\n",
            "Epoch 71/250, Loss: 1.3659\n",
            "Epoch 72/250, Loss: 1.3536\n",
            "Epoch 73/250, Loss: 1.3416\n",
            "Epoch 74/250, Loss: 1.3299\n",
            "Epoch 75/250, Loss: 1.3184\n",
            "Epoch 76/250, Loss: 1.3073\n",
            "Epoch 77/250, Loss: 1.2964\n",
            "Epoch 78/250, Loss: 1.2858\n",
            "Epoch 79/250, Loss: 1.2755\n",
            "Epoch 80/250, Loss: 1.2654\n",
            "Epoch 81/250, Loss: 1.2556\n",
            "Epoch 82/250, Loss: 1.2460\n",
            "Epoch 83/250, Loss: 1.2367\n",
            "Epoch 84/250, Loss: 1.2277\n",
            "Epoch 85/250, Loss: 1.2188\n",
            "Epoch 86/250, Loss: 1.2102\n",
            "Epoch 87/250, Loss: 1.2018\n",
            "Epoch 88/250, Loss: 1.1936\n",
            "Epoch 89/250, Loss: 1.1855\n",
            "Epoch 90/250, Loss: 1.1777\n",
            "Epoch 91/250, Loss: 1.1700\n",
            "Epoch 92/250, Loss: 1.1625\n",
            "Epoch 93/250, Loss: 1.1551\n",
            "Epoch 94/250, Loss: 1.1479\n",
            "Epoch 95/250, Loss: 1.1408\n",
            "Epoch 96/250, Loss: 1.1339\n",
            "Epoch 97/250, Loss: 1.1271\n",
            "Epoch 98/250, Loss: 1.1204\n",
            "Epoch 99/250, Loss: 1.1139\n",
            "Epoch 100/250, Loss: 1.1074\n",
            "Epoch 101/250, Loss: 1.1011\n",
            "Epoch 102/250, Loss: 1.0949\n",
            "Epoch 103/250, Loss: 1.0889\n",
            "Epoch 104/250, Loss: 1.0829\n",
            "Epoch 105/250, Loss: 1.0770\n",
            "Epoch 106/250, Loss: 1.0712\n",
            "Epoch 107/250, Loss: 1.0655\n",
            "Epoch 108/250, Loss: 1.0599\n",
            "Epoch 109/250, Loss: 1.0544\n",
            "Epoch 110/250, Loss: 1.0490\n",
            "Epoch 111/250, Loss: 1.0436\n",
            "Epoch 112/250, Loss: 1.0383\n",
            "Epoch 113/250, Loss: 1.0331\n",
            "Epoch 114/250, Loss: 1.0280\n",
            "Epoch 115/250, Loss: 1.0229\n",
            "Epoch 116/250, Loss: 1.0180\n",
            "Epoch 117/250, Loss: 1.0130\n",
            "Epoch 118/250, Loss: 1.0082\n",
            "Epoch 119/250, Loss: 1.0034\n",
            "Epoch 120/250, Loss: 0.9987\n",
            "Epoch 121/250, Loss: 0.9941\n",
            "Epoch 122/250, Loss: 0.9896\n",
            "Epoch 123/250, Loss: 0.9851\n",
            "Epoch 124/250, Loss: 0.9806\n",
            "Epoch 125/250, Loss: 0.9763\n",
            "Epoch 126/250, Loss: 0.9720\n",
            "Epoch 127/250, Loss: 0.9677\n",
            "Epoch 128/250, Loss: 0.9636\n",
            "Epoch 129/250, Loss: 0.9594\n",
            "Epoch 130/250, Loss: 0.9554\n",
            "Epoch 131/250, Loss: 0.9513\n",
            "Epoch 132/250, Loss: 0.9474\n",
            "Epoch 133/250, Loss: 0.9434\n",
            "Epoch 134/250, Loss: 0.9395\n",
            "Epoch 135/250, Loss: 0.9357\n",
            "Epoch 136/250, Loss: 0.9319\n",
            "Epoch 137/250, Loss: 0.9282\n",
            "Epoch 138/250, Loss: 0.9245\n",
            "Epoch 139/250, Loss: 0.9208\n",
            "Epoch 140/250, Loss: 0.9172\n",
            "Epoch 141/250, Loss: 0.9136\n",
            "Epoch 142/250, Loss: 0.9100\n",
            "Epoch 143/250, Loss: 0.9065\n",
            "Epoch 144/250, Loss: 0.9030\n",
            "Epoch 145/250, Loss: 0.8995\n",
            "Epoch 146/250, Loss: 0.8961\n",
            "Epoch 147/250, Loss: 0.8927\n",
            "Epoch 148/250, Loss: 0.8893\n",
            "Epoch 149/250, Loss: 0.8860\n",
            "Epoch 150/250, Loss: 0.8828\n",
            "Epoch 151/250, Loss: 0.8795\n",
            "Epoch 152/250, Loss: 0.8763\n",
            "Epoch 153/250, Loss: 0.8731\n",
            "Epoch 154/250, Loss: 0.8700\n",
            "Epoch 155/250, Loss: 0.8669\n",
            "Epoch 156/250, Loss: 0.8638\n",
            "Epoch 157/250, Loss: 0.8607\n",
            "Epoch 158/250, Loss: 0.8577\n",
            "Epoch 159/250, Loss: 0.8547\n",
            "Epoch 160/250, Loss: 0.8517\n",
            "Epoch 161/250, Loss: 0.8488\n",
            "Epoch 162/250, Loss: 0.8459\n",
            "Epoch 163/250, Loss: 0.8430\n",
            "Epoch 164/250, Loss: 0.8402\n",
            "Epoch 165/250, Loss: 0.8373\n",
            "Epoch 166/250, Loss: 0.8346\n",
            "Epoch 167/250, Loss: 0.8318\n",
            "Epoch 168/250, Loss: 0.8290\n",
            "Epoch 169/250, Loss: 0.8263\n",
            "Epoch 170/250, Loss: 0.8236\n",
            "Epoch 171/250, Loss: 0.8210\n",
            "Epoch 172/250, Loss: 0.8183\n",
            "Epoch 173/250, Loss: 0.8157\n",
            "Epoch 174/250, Loss: 0.8131\n",
            "Epoch 175/250, Loss: 0.8106\n",
            "Epoch 176/250, Loss: 0.8080\n",
            "Epoch 177/250, Loss: 0.8055\n",
            "Epoch 178/250, Loss: 0.8030\n",
            "Epoch 179/250, Loss: 0.8006\n",
            "Epoch 180/250, Loss: 0.7981\n",
            "Epoch 181/250, Loss: 0.7957\n",
            "Epoch 182/250, Loss: 0.7933\n",
            "Epoch 183/250, Loss: 0.7909\n",
            "Epoch 184/250, Loss: 0.7886\n",
            "Epoch 185/250, Loss: 0.7863\n",
            "Epoch 186/250, Loss: 0.7840\n",
            "Epoch 187/250, Loss: 0.7817\n",
            "Epoch 188/250, Loss: 0.7794\n",
            "Epoch 189/250, Loss: 0.7772\n",
            "Epoch 190/250, Loss: 0.7750\n",
            "Epoch 191/250, Loss: 0.7728\n",
            "Epoch 192/250, Loss: 0.7706\n",
            "Epoch 193/250, Loss: 0.7685\n",
            "Epoch 194/250, Loss: 0.7664\n",
            "Epoch 195/250, Loss: 0.7642\n",
            "Epoch 196/250, Loss: 0.7622\n",
            "Epoch 197/250, Loss: 0.7601\n",
            "Epoch 198/250, Loss: 0.7580\n",
            "Epoch 199/250, Loss: 0.7560\n",
            "Epoch 200/250, Loss: 0.7540\n",
            "Epoch 201/250, Loss: 0.7520\n",
            "Epoch 202/250, Loss: 0.7500\n",
            "Epoch 203/250, Loss: 0.7481\n",
            "Epoch 204/250, Loss: 0.7461\n",
            "Epoch 205/250, Loss: 0.7442\n",
            "Epoch 206/250, Loss: 0.7423\n",
            "Epoch 207/250, Loss: 0.7404\n",
            "Epoch 208/250, Loss: 0.7385\n",
            "Epoch 209/250, Loss: 0.7366\n",
            "Epoch 210/250, Loss: 0.7348\n",
            "Epoch 211/250, Loss: 0.7330\n",
            "Epoch 212/250, Loss: 0.7311\n",
            "Epoch 213/250, Loss: 0.7293\n",
            "Epoch 214/250, Loss: 0.7275\n",
            "Epoch 215/250, Loss: 0.7258\n",
            "Epoch 216/250, Loss: 0.7240\n",
            "Epoch 217/250, Loss: 0.7223\n",
            "Epoch 218/250, Loss: 0.7205\n",
            "Epoch 219/250, Loss: 0.7188\n",
            "Epoch 220/250, Loss: 0.7171\n",
            "Epoch 221/250, Loss: 0.7154\n",
            "Epoch 222/250, Loss: 0.7137\n",
            "Epoch 223/250, Loss: 0.7120\n",
            "Epoch 224/250, Loss: 0.7104\n",
            "Epoch 225/250, Loss: 0.7087\n",
            "Epoch 226/250, Loss: 0.7071\n",
            "Epoch 227/250, Loss: 0.7055\n",
            "Epoch 228/250, Loss: 0.7038\n",
            "Epoch 229/250, Loss: 0.7022\n",
            "Epoch 230/250, Loss: 0.7007\n",
            "Epoch 231/250, Loss: 0.6991\n",
            "Epoch 232/250, Loss: 0.6975\n",
            "Epoch 233/250, Loss: 0.6959\n",
            "Epoch 234/250, Loss: 0.6944\n",
            "Epoch 235/250, Loss: 0.6929\n",
            "Epoch 236/250, Loss: 0.6913\n",
            "Epoch 237/250, Loss: 0.6898\n",
            "Epoch 238/250, Loss: 0.6883\n",
            "Epoch 239/250, Loss: 0.6868\n",
            "Epoch 240/250, Loss: 0.6853\n",
            "Epoch 241/250, Loss: 0.6838\n",
            "Epoch 242/250, Loss: 0.6824\n",
            "Epoch 243/250, Loss: 0.6809\n",
            "Epoch 244/250, Loss: 0.6795\n",
            "Epoch 245/250, Loss: 0.6780\n",
            "Epoch 246/250, Loss: 0.6766\n",
            "Epoch 247/250, Loss: 0.6752\n",
            "Epoch 248/250, Loss: 0.6737\n",
            "Epoch 249/250, Loss: 0.6723\n",
            "Epoch 250/250, Loss: 0.6709\n",
            "Final Training Accuracy: 76.43%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 73.30%\n",
            "Final Training Accuracy: 76.43%\n",
            "Final Test Accuracy: 73.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SJEDldwmf4h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acU8UXUff4mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DusoIWjf4qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DR2btKNf4td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bm9xiVROf4xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiHiTvMDuM90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}