# -*- coding: utf-8 -*-
"""problem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKJ7pp9h0xVpcIyq1EyAnypAJBImV4SQ
"""

import numpy as np
import pandas as pd
from keras.datasets import fashion_mnist

(x_train, y_train), (x_test, y_test)= fashion_mnist.load_data()      #loading the dataset

x_train= x_train.reshape(x_train.shape[0], -1)                        #reshape the x into (60000, 784)
y= y_train                                                            #y is already a 1D array

x= x_train.reshape(x_train.shape[0], -1)/255.0                        #normalizing x

def sigmoid(z):                             #define sigmoid function
  return 1/(1+np.exp(-z))

def softmax(z):
  return np.exp(z)/ np.sum(np.exp(z), axis= 0)   #softmax function

# initialization of weights and bias vector
def initialize_weights_bias(layer_size):
  all_parameters= {}           #empty dictionary to store weights & biases
  L= len(layer_size) -1        #exclude the input vector

  for i in range(1, L+1):
    all_parameters[f"W{i}"] = np.random.randn(layer_size[i], layer_size[i-1])    #initialize W randomly
    all_parameters[f"b{i}"] = np.random.randn(layer_size[i], 1)                  #initialize b randomly, column vector, one bias per neuron
  return all_parameters

#define forward propagation
def forward_propagation(x, y, num_of_hidden_layers, num_of_neurons):
  N= len(np.unique(y))
  input_dimension = x.shape[1]                           #number of features per sample
  layer_size = [input_dimension] + [num_of_neurons] * num_of_hidden_layers + [N]

  all_parameters = initialize_weights_bias(layer_size)    #calling weights and biases

  pre_activations = {}
  activations = {"h0": x.T}                                #input layer activation h0 is input x itself




  for i in range(1, num_of_hidden_layers +1 ):      #calculate weights and biases for hidden layers
    a_i= np.dot(all_parameters[f"W{i}"], activations[f"h{i-1}"]) + all_parameters[f"b{i}"]
    h_i= sigmoid(a_i)

    pre_activations[f"a{i}"]= a_i
    activations[f"h{i}"]= h_i

                                                    #output layer preactivation and y_cap= h_L
  L= num_of_hidden_layers + 1                       #calculate weights and biases for output layer
  a_L= np.dot(all_parameters[f"W{L}"], activations[f"h{L - 1}"]) + all_parameters[f"b{L}"]
  h_L= softmax(a_L)                                 #output in last layer or y_cap

  pre_activations[f"a{L}"]= a_L
  activations[f"h{L}"] = h_L

  return h_L.T

y_cap= forward_propagation(x, y, 5, 70)           #feed forward function call

print(f"\n‚úÖ Predicted output shape: {y_cap.shape} ")
print("\nüîç Predicted probability distribution of 10 classes on first sample of dataset :\n", y_cap[0])