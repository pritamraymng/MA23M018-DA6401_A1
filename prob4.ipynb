{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHEMTsD3uVYQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from types import SimpleNamespace\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer class\n",
        "class Optimizer:\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, epsilon=1e-4, beta=0.9, beta1=0.9, beta2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "\n",
        "    def sgd(self, parameters, grads, prev_u=None):\n",
        "\n",
        "        for t in parameters.keys():\n",
        "            parameters[t] -= self.learning_rate * grads[t]\n",
        "        return parameters, prev_u\n",
        "\n",
        "    def momentum_gradient_descent(self, parameters, grads, prev_u=None, beta=0.9):\n",
        "\n",
        "        if prev_u is None:\n",
        "            prev_u = {key: np.zeros_like(value) for key, value in parameters.items()}         # Initialize with zeros (u(-1)= 0)\n",
        "        for t in parameters.keys():\n",
        "            prev_u[t] = beta * prev_u[t] + grads[t]\n",
        "            parameters[t] -= self.learning_rate * prev_u[t]                                   #prev_u[t] accumuklates the history vector\n",
        "        return parameters, prev_u\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, parameters, grads, prev_u=None, beta=0.9):\n",
        "\n",
        "        if prev_u is None:\n",
        "            prev_u = {t: np.zeros_like(value) for t, value in parameters.items()}             # initialize u(-1)= 0\n",
        "        lookahead_params = {t: parameters[t] - beta * prev_u[t] for t in parameters.keys()}\n",
        "        temp_grads = grads\n",
        "        for t in parameters.keys():\n",
        "            prev_u[t] = beta * prev_u[t] + self.learning_rate * temp_grads[t]                     #prev_u[t] accumuklates the history vector\n",
        "            parameters[t] -= prev_u[t]\n",
        "        return parameters, prev_u\n",
        "\n",
        "    def rmsprop(self, parameters, grads, prev_u=None, beta=0.9):\n",
        "\n",
        "        if prev_u is None:\n",
        "            prev_u = {t: np.zeros_like(value) for t, value in parameters.items()}\n",
        "        for t in parameters.keys():\n",
        "            prev_u[t] = beta * prev_u[t] + (1 - beta) * grads[t] ** 2\n",
        "            parameters[t] -= (self.learning_rate / (np.sqrt(prev_u[t]) + self.epsilon)) * grads[t]\n",
        "        return parameters, prev_u\n",
        "\n",
        "    def adam(self, parameters, grads, prev_u=None):\n",
        "\n",
        "        if prev_u is None or \"step\" not in prev_u:                                  #initialization\n",
        "            prev_u = {\"m\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"v\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"step\": 0}\n",
        "        prev_u[\"step\"] += 1\n",
        "        step = prev_u[\"step\"]\n",
        "        for t in parameters.keys():\n",
        "            prev_u[\"m\"][t] = self.beta1 * prev_u[\"m\"][t] + (1 - self.beta1) * grads[t]         #bias corrected moment estimate\n",
        "            prev_u[\"v\"][t] = self.beta2 * prev_u[\"v\"][t] + (1 - self.beta2) * (grads[t] ** 2)\n",
        "            m_hat = prev_u[\"m\"][t] / (1 - self.beta1 ** step)\n",
        "            v_hat = prev_u[\"v\"][t] / (1 - self.beta2 ** step)\n",
        "            parameters[t] -= (self.learning_rate * m_hat) / (np.sqrt(v_hat) + self.epsilon)     #update rule\n",
        "        return parameters, prev_u\n",
        "\n",
        "    def nadam(self, parameters, grads, prev_u=None):\n",
        "\n",
        "        if prev_u is None or \"step\" not in prev_u:\n",
        "            prev_u = {\"m\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"v\": {t: np.zeros_like(value) for t, value in parameters.items()},\n",
        "                      \"step\": 0}\n",
        "        prev_u[\"step\"] += 1\n",
        "        step = prev_u[\"step\"]\n",
        "        for t in parameters.keys():\n",
        "            prev_u[\"m\"][t] = self.beta1 * prev_u[\"m\"][t] + (1 - self.beta1) * grads[t]          #bias corrected moment estimate\n",
        "            prev_u[\"v\"][t] = self.beta2 * prev_u[\"v\"][t] + (1 - self.beta2) * (grads[t] ** 2)\n",
        "            m_hat = prev_u[\"m\"][t] / (1 - self.beta1 ** step)\n",
        "            v_hat = prev_u[\"v\"][t] / (1 - self.beta2 ** step)\n",
        "            nadam_update = (self.beta1 * m_hat +\n",
        "                            (1 - self.beta1) * grads[t] / (1 - self.beta1 ** step))\n",
        "            parameters[t] -= (self.learning_rate / (np.sqrt(v_hat) + self.epsilon)) * nadam_update    #update rule\n",
        "        return parameters, prev_u\n"
      ],
      "metadata": {
        "id": "Sdvs3vAZvhbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, weight_init=\"random\", activation=\"sigmoid\",beta=0.9, beta1=0.9, beta2=0.999, weight_decay=0.0):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.weight_init = weight_init\n",
        "        self.activation = activation\n",
        "        self.weight_decay = weight_decay\n",
        "        self.parameters = self.initialize_weights_bias(layer_sizes)\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "\n",
        "    #activation functions\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "    def initialize_weights_bias(self, layer_size):          #weight and biases initializes\n",
        "        parameters = {}          #empty dictionary to strore parameters(weights, biases)\n",
        "        L = len(layer_size) - 1\n",
        "        for i in range(1, L + 1):\n",
        "            if self.weight_init == \"xavier\":\n",
        "                parameters[f\"W{i}\"] = (np.random.randn(layer_size[i], layer_size[i-1]) *np.sqrt(2 / (layer_size[i] + layer_size[i-1])))\n",
        "            else:               #random initialization\n",
        "                parameters[f\"W{i}\"] = np.random.randn(layer_size[i], layer_size[i-1])\n",
        "            parameters[f\"b{i}\"] = np.zeros((layer_size[i], 1))\n",
        "        return parameters\n",
        "\n",
        "\n",
        "    def one_hot_encode(self, y, num_classes):\n",
        "        y = y.astype(int).flatten()\n",
        "        one_hot = np.zeros((y.shape[0], num_classes))\n",
        "        one_hot[np.arange(y.shape[0]), y] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        # Combine into a single dataset\n",
        "        x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "        x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "\n",
        "\n",
        "        print(\"Fashion MNIST Dataset Details\")\n",
        "        print(\"=\" * 20)\n",
        "        print(f\"Training Samples: {x_train.shape[0]}\")\n",
        "        print(f\"Test Samples: {x_test.shape[0]}\")\n",
        "        print(f\"Input Features: {x_train.shape[1]}\")\n",
        "        print(\"=\" * 200)\n",
        "\n",
        "        return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "    def split_data(self):\n",
        "        (x_train, y_train_raw), (x_test, y_test_raw) = self.load_and_preprocess_data()\n",
        "\n",
        "          # Further split training data into train and validation sets (10% validation)\n",
        "        x_train, x_val, y_train_raw, y_val_raw = train_test_split(x_train, y_train_raw, test_size=0.1, random_state=42)\n",
        "\n",
        "         # One-hot encoding labels\n",
        "        num_classes = len(np.unique(y_train_raw))\n",
        "        self.num_classes = num_classes\n",
        "        y_train_enc = self.one_hot_encode(y_train_raw, num_classes)\n",
        "        y_val_enc = self.one_hot_encode(y_val_raw, num_classes)\n",
        "        y_test_enc = self.one_hot_encode(y_test_raw, num_classes)\n",
        "        print(\"=\" * 40)\n",
        "        print(\" Fashion MNIST Dataset Train, Validation, Test split\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Training Samples: {x_train.shape[0]}\")\n",
        "        print(f\"Validation Samples: {x_val.shape[0]}\")\n",
        "        print(f\"Test Samples: {x_test.shape[0]}\")\n",
        "\n",
        "        return x_train, y_train_raw, y_train_enc, x_val, y_val_raw, y_val_enc, x_test, y_test_raw, y_test_enc\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propagation(self, x_batch, y_batch_raw, num_of_hidden_layers, num_neurons):\n",
        "        pre_activations = {}\n",
        "        activations = {\"h0\": x_batch.T}\n",
        "        for i in range(1, num_of_hidden_layers + 1):\n",
        "            a_i = np.dot(self.parameters[f\"W{i}\"], activations[f\"h{i-1}\"]) + self.parameters[f\"b{i}\"]\n",
        "            if self.activation == \"sigmoid\":\n",
        "                h_i = self.sigmoid(a_i)\n",
        "            elif self.activation == \"relu\":\n",
        "                h_i = self.relu(a_i)\n",
        "            elif self.activation == \"tanh\":\n",
        "                h_i = self.tanh(a_i)\n",
        "            else:\n",
        "                raise ValueError(\"activation function not defined\")\n",
        "            pre_activations[f\"a{i}\"] = a_i\n",
        "            activations[f\"h{i}\"] = h_i\n",
        "\n",
        "        # Output layer\n",
        "        L = num_of_hidden_layers + 1\n",
        "        a_L = np.dot(self.parameters[f\"W{L}\"], activations[f\"h{L-1}\"]) + self.parameters[f\"b{L}\"]\n",
        "        h_L = self.softmax(a_L)      #y_hat\n",
        "        pre_activations[f\"a{L}\"] = a_L\n",
        "        activations[f\"h{L}\"] = h_L\n",
        "        return h_L.T, pre_activations, activations\n",
        "\n",
        "\n",
        "    def backpropagation(self, x_batch, y_batch_enc, pre_activations, activations,optimizer, batch_size, prev_v, weight_decay):\n",
        "\n",
        "        grads = {}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "\n",
        "        # talk to the last layer\n",
        "        nabla_a_L = activations[f\"h{L}\"].T - y_batch_enc                 #(batch_size, num_classes)\n",
        "\n",
        "        # Output layer gradients\n",
        "        grads[f\"W{L}\"] = (np.dot(nabla_a_L.T, activations[f\"h{L-1}\"].T)\n",
        "                          + weight_decay * self.parameters[f\"W{L}\"]) / batch_size\n",
        "        grads[f\"b{L}\"] = np.sum(nabla_a_L, axis=0, keepdims=True).T / batch_size\n",
        "\n",
        "        # Talk to hidden layers\n",
        "        for k in range(L-1, 0, -1):\n",
        "            nabla_h_k = np.dot(self.parameters[f\"W{k+1}\"].T, nabla_a_L.T)\n",
        "            if self.activation == \"sigmoid\":\n",
        "                nabla_a_k = nabla_h_k * self.sigmoid_derivative(pre_activations[f\"a{k}\"])\n",
        "            elif self.activation == 'relu':\n",
        "                nabla_a_k = nabla_h_k * self.relu_derivative(pre_activations[f\"a{k}\"])\n",
        "            elif self.activation == \"tanh\":\n",
        "                nabla_a_k = nabla_h_k * self.tanh_derivative(pre_activations[f\"a{k}\"])\n",
        "            else:\n",
        "                raise ValueError(\"Activation not defined.\")\n",
        "\n",
        "            grads[f\"W{k}\"] = (np.dot(nabla_a_k, activations[f\"h{k-1}\"].T)+ weight_decay * self.parameters[f\"W{k}\"]) / batch_size\n",
        "            grads[f\"b{k}\"] = np.sum(nabla_a_k, axis=1, keepdims=True) / batch_size\n",
        "            nabla_a_L = nabla_a_k.T\n",
        "\n",
        "        # Apply chosen optimizer function\n",
        "        if isinstance(optimizer, Optimizer):\n",
        "            raise ValueError(\"Should be  an optimizer method sgd, adam) etc.\")\n",
        "        self.parameters, prev_v = optimizer(self.parameters, grads, prev_v)\n",
        "        return self.parameters, prev_v\n",
        "\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred, batch_size, weight_decay):\n",
        "        epsilon = 1e-8\n",
        "        l2_norm = sum(np.sum(np.square(self.parameters[f\"W{i}\"])) for i in range(1, len(self.layer_sizes)))\n",
        "        cross_entropy = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "        return cross_entropy + 0.5 * weight_decay * l2_norm\n",
        "\n",
        "\n",
        "    def plot_loss_chart(self, epochs, train_loss, val_loss):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(epochs, train_loss, marker='o', label=\"Training Loss\")\n",
        "        plt.plot(epochs, val_loss, marker='o', label=\"Validation Loss\")\n",
        "        plt.title(\"Loss vs. Epoch\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        return plt.gcf()           #current figure\n",
        "\n",
        "\n",
        "    def train_model(self, x_train, y_train_raw, y_train_enc,x_val, y_val_raw, y_val_enc,num_hidden_layers, num_neurons, optimizer_type,\n",
        "                    learning_rate, num_epochs, batch_size, weight_decay):\n",
        "\n",
        "\n",
        "        self.layer_sizes = [x_train.shape[1]] + [num_neurons]*num_hidden_layers + [self.num_classes]\n",
        "        # initialize\n",
        "        self.parameters = self.initialize_weights_bias(self.layer_sizes)\n",
        "\n",
        "        # Instantiate optimizer\n",
        "        opt = Optimizer(learning_rate=learning_rate, beta=self.beta, beta1=self.beta1, beta2=self.beta2)\n",
        "        if optimizer_type == 'sgd':\n",
        "            optimizer_function = opt.sgd\n",
        "        elif optimizer_type == 'momentum':\n",
        "            optimizer_function = opt.momentum_gradient_descent\n",
        "        elif optimizer_type == 'nesterov':\n",
        "            optimizer_function = opt.nesterov_accelerated_gradient_descent\n",
        "        elif optimizer_type == 'rmsprop':\n",
        "            optimizer_function = opt.rmsprop\n",
        "        elif optimizer_type == 'adam':\n",
        "            optimizer_function = opt.adam\n",
        "        elif optimizer_type == 'nadam':\n",
        "            optimizer_function = opt.nadam\n",
        "        else:\n",
        "            raise ValueError(f\"optimizer not defined: {optimizer_type}\")\n",
        "\n",
        "        prev_u = None\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        n_samples = x_train.shape[0]\n",
        "\n",
        "\n",
        "        for epoch in range(1, num_epochs+1):\n",
        "            epoch_loss = 0.0\n",
        "            # Mini-batch training\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_x = x_train[i:i+batch_size, :]\n",
        "                batch_enc = y_train_enc[i:i+batch_size, :]\n",
        "                # Forward propagation\n",
        "                y_hat, pre_acts, acts = self.forward_propagation(batch_x, None, num_hidden_layers, num_neurons)\n",
        "                # Backward prop & parameter update\n",
        "                self.parameters, prev_u = self.backpropagation(batch_x, batch_enc, pre_acts, acts,optimizer_function, batch_x.shape[0],prev_u, weight_decay)\n",
        "                # Accumulate loss\n",
        "                epoch_loss += self.compute_loss(batch_enc, y_hat, batch_x.shape[0], weight_decay)\n",
        "\n",
        "            # Average training loss for this epoch\n",
        "            avg_train_loss = epoch_loss / (n_samples / batch_size)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Validation\n",
        "            y_hat_val, _, _ = self.forward_propagation(x_val, None, num_hidden_layers, num_neurons)\n",
        "            val_loss = self.compute_loss(y_val_enc, y_hat_val, x_val.shape[0], weight_decay)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Compute training accuracy\n",
        "            y_hat_full, _, _ = self.forward_propagation(x_train, None, num_hidden_layers, num_neurons)\n",
        "            train_acc = np.mean(np.argmax(y_hat_full, axis=1) == y_train_raw)\n",
        "\n",
        "            # Compute validation accuracy\n",
        "            val_acc = np.mean(np.argmax(y_hat_val, axis=1) == y_val_raw)\n",
        "\n",
        "            # Log to Weights & Biases\n",
        "            wandb.log({\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"epoch\": epoch\n",
        "            })\n",
        "\n",
        "            print(f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
        "                  f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "        # Plot\n",
        "        epochs_range = list(range(1, num_epochs + 1))\n",
        "        fig = self.plot_loss_chart(epochs_range, train_losses, val_losses)\n",
        "        plt.show()\n",
        "\n",
        "        return self.parameters, train_losses, val_losses\n",
        "\n",
        "\n",
        "\n",
        "    def test_model(self, x_test, y_test_raw, y_test_enc, num_hidden_layers, num_neurons, batch_size):\n",
        "        y_hat, _, _ = self.forward_propagation(x_test, None, num_hidden_layers, num_neurons)\n",
        "        test_loss = self.compute_loss(y_test_enc, y_hat, x_test.shape[0], self.weight_decay)\n",
        "        y_hat_labels = np.argmax(y_hat, axis=1)\n",
        "        test_accuracy = np.mean(y_hat_labels == y_test_raw)\n",
        "\n",
        "        print(f\"Test Loss: {test_loss:.4f} \\tTest Accuracy: {test_accuracy:.4f}\")\n",
        "        return test_loss, test_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "nWBe93Havl1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sweep configuration\n",
        "sweep_config = {\n",
        "    \"name\": \"sweep 100 runs\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},         #maximize validation accuracy\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [5, 10]},\n",
        "        \"num_hidden_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"num_neurons\": {\"values\": [32, 64, 128]},\n",
        "        \"weight_decay\": {\"values\": [0, 0.0005, 0.5]},\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"weight_init\": {\"values\": [\"random\", \"xavier\"]},\n",
        "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"relu\"]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"A1_MA23M018\")    #https://wandb.ai/pritamapcr-indian-institute-of-technology-madras/A1_MA23M018/sweeps/wv78yn01\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "EXUJGtO4wQC8",
        "outputId": "31667a91-8879-4e4b-8ca6-de9af893c157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: wv78yn01\n",
            "Sweep URL: https://wandb.ai/pritamapcr-indian-institute-of-technology-madras/A1_MA23M018/sweeps/wv78yn01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "        run_name = (f\"hl_{config.num_hidden_layers}_bs_{config.batch_size}_\"\n",
        "                    f\"ac_{config.activation}_ep_{config.epochs}\")\n",
        "        wandb.run.name = run_name\n",
        "\n",
        "        # Initialize the neural network\n",
        "        input_size = 28 * 28\n",
        "        layer_sizes = [input_size] + [config.num_neurons] * config.num_hidden_layers\n",
        "        nn = NeuralNetwork(\n",
        "            layer_sizes=layer_sizes,\n",
        "            weight_init=config.weight_init,\n",
        "            activation=config.activation,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Load and split the dataset\n",
        "        (X_train, Y_train_raw, Y_train_enc,\n",
        "         X_val, Y_val_raw, Y_val_enc,\n",
        "         X_test, Y_test_raw, Y_test_enc) = nn.split_data()\n",
        "\n",
        "        # Train the model\n",
        "        final_params, train_losses, val_losses = nn.train_model(\n",
        "            x_train=X_train,\n",
        "            y_train_raw=Y_train_raw,\n",
        "            y_train_enc=Y_train_enc,\n",
        "            x_val=X_val,\n",
        "            y_val_raw=Y_val_raw,\n",
        "            y_val_enc=Y_val_enc,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_neurons=config.num_neurons,\n",
        "            optimizer_type=config.optimizer,\n",
        "            learning_rate=config.learning_rate,\n",
        "            num_epochs=config.epochs,\n",
        "            batch_size=config.batch_size,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Test the model\n",
        "        test_loss, test_acc = nn.test_model(\n",
        "            X_test, Y_test_raw, Y_test_enc,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_neurons=config.num_neurons,\n",
        "            batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        # validation accuracy\n",
        "        y_hat_val, _, _ = nn.forward_propagation(X_val, None, config.num_hidden_layers, config.num_neurons)\n",
        "        final_val_acc = np.mean(np.argmax(y_hat_val, axis=1) == Y_val_raw)\n",
        "\n",
        "        wandb.log({\n",
        "            \"final_train_loss\": train_losses[-1],\n",
        "            \"final_val_loss\": val_losses[-1],\n",
        "            \"test_loss\": test_loss,\n",
        "            \"test_accuracy\": test_acc,\n",
        "            \"val_acc\": final_val_acc  # final validation accuracy\n",
        "        })\n",
        "\n",
        "        # run summary\n",
        "        run.summary[\"val_acc\"] = final_val_acc\n",
        "        run.summary[\"test_accuracy\"] = test_acc\n",
        "        run.summary[\"hyperparameters\"] = dict(config)\n",
        "\n",
        "        # Print\n",
        "        print(\"\\nRun Summary:\")\n",
        "        print(\"Hyperparameters:\", dict(config))\n",
        "        print(f\"Final Validation Accuracy: {final_val_acc}\")\n",
        "        print(f\"Final Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Run the sweep agent for 100 trials\n",
        "wandb.agent(sweep_id, function=main, count=100)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "c68uJNEHwobc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}